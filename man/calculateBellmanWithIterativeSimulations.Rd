% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterations_simulation_DP.R
\name{calculateBellmanWithIterativeSimulations}
\alias{calculateBellmanWithIterativeSimulations}
\title{Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation}
\usage{
calculateBellmanWithIterativeSimulations(
  area,
  pumping,
  pump_eff = 1,
  opts,
  nb_control = 10,
  nb_itr = 3,
  mcyears,
  penalty_low,
  penalty_high,
  path_solver,
  study_path,
  hours = round(seq(0, 168, length.out = 10)),
  states_step_ratio = 1/50
)
}
\arguments{
\item{area}{Area with the reservoir}

\item{pumping}{Binary, T if pumping is possible}

\item{pump_eff}{Pumping efficiency (1 if no pumping)}

\item{opts}{List of simulation parameters returned by the function
\code{antaresRead::setSimulationPath}}

\item{nb_control}{Number of controls used in the interpolation of the reward function}

\item{nb_itr}{Max number of iterations}

\item{mcyears}{Vector of years used to evaluate rewards}

\item{penalty_low}{Penalty for violating the bottom rule curve, comparable to the unsupplied energy cost}

\item{penalty_high}{Penalty for violating the top rule curve, comparable to the spilled energy cost}

\item{path_solver}{Character containing the Antares Solver path, argument passed to \code{\link[antaresEditObject]{runSimulation}}.}

\item{study_path}{Character containing the Antares study}

\item{hours}{Vector of hours used to evaluate costs/rewards of pumping/generating}

\item{states_step_ratio}{Discretization ratio to generate steps levels
between the reservoir capacity and zero}
}
\value{
List containing aggregated water values and the data table with all years for the last iteration
}
\description{
Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterations_simulation_DP.R
\name{calculateBellmanWithIterativeSimulationsMultiStock}
\alias{calculateBellmanWithIterativeSimulationsMultiStock}
\title{Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation}
\usage{
calculateBellmanWithIterativeSimulationsMultiStock(
  list_areas,
  list_pumping,
  list_eff,
  opts,
  nb_control = 10,
  nb_itr = 3,
  mcyears,
  penalty_low,
  penalty_high,
  path_solver,
  study_path,
  states_step_ratio = 1/50,
  method_dp = "grid-mean",
  q_ratio = 0.5,
  method_fast = F,
  test_vu = F,
  force_final_level = F,
  final_level_egal_initial = F,
  final_level = NULL,
  penalty_final_level = NULL,
  initial_traj = NULL
)
}
\arguments{
\item{list_areas}{List of areas concerned by the simulation.}

\item{list_pumping}{List of bools to tell if pumping is available in areas}

\item{opts}{List of simulation parameters returned by the function
\code{antaresRead::setSimulationPath}}

\item{nb_control}{Number of controls used in the interpolation of the reward function}

\item{nb_itr}{Max number of iterations}

\item{mcyears}{Vector of years used to evaluate rewards}

\item{penalty_low}{Penalty for violating the bottom rule curve, comparable to the unsupplied energy cost}

\item{penalty_high}{Penalty for violating the top rule curve, comparable to the spilled energy cost}

\item{path_solver}{Character containing the Antares Solver path, argument passed to \code{\link[antaresEditObject]{runSimulation}}.}

\item{study_path}{Character containing the Antares study}

\item{states_step_ratio}{Discretization ratio to generate steps levels
between the reservoir capacity and zero}

\item{method_dp}{Algorithm in dynamic programming part}

\item{q_ratio}{from 0 to 1. the probability used in quantile method
to determine a bellman value which q_ratio all bellman values are equal or
less to it. (quantile(q_ratio))}

\item{method_fast}{Method to choose evaluated controls}

\item{test_vu}{Binary. If you want to run a Antares simulation between each iteration
with the latest water values}

\item{force_final_level}{Binary. Whether final level should be constrained}

\item{final_level_egal_initial}{Binary. Whether final level, if constrained, should be equal to initial level}

\item{final_level}{Final level (in percent between 0 and 100) if final level is constrained but different from initial level}

\item{penalty_final_level}{Penalties (for both bottom and top rule curves) to constrain final level}

\item{initial_traj}{Initial trajectory}
}
\value{
List containing aggregated water values and the data table with all years for the last iteration
}
\description{
Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation
}

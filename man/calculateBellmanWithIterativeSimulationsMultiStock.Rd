% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterations_simulation_DP.R
\name{calculateBellmanWithIterativeSimulationsMultiStock}
\alias{calculateBellmanWithIterativeSimulationsMultiStock}
\title{Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation}
\usage{
calculateBellmanWithIterativeSimulationsMultiStock(
  list_areas,
  list_pumping,
  list_efficiency,
  opts,
  nb_control = 10,
  nb_itr = 3,
  mcyears,
  penalty_low,
  penalty_high,
  path_solver,
  states_step_ratio = 1/50,
  cvar_value = 1,
  force_final_level = FALSE,
  penalty_final_level = NULL,
  initial_traj = NULL,
  df_previous_cut = NULL,
  list_areas_to_compute = NULL
)
}
\arguments{
\item{list_areas}{Vector of areas concerned by simulations.}

\item{list_pumping}{Named vector of binary to tell if pumping is available in areas.}

\item{list_efficiency}{Named vector of pumping efficiency.}

\item{opts}{List of simulation parameters returned by the function
\code{antaresRead::setSimulationPath}}

\item{nb_control}{Number of controls used in the interpolation of the reward function}

\item{nb_itr}{Max number of iterations}

\item{mcyears}{Vector of years used to evaluate rewards}

\item{penalty_low}{Penalty for violating the bottom rule curve, comparable to the unsupplied energy cost}

\item{penalty_high}{Penalty for violating the top rule curve, comparable to the spilled energy cost}

\item{path_solver}{Character containing the Antares Solver path, argument passed to \code{\link[antaresEditObject]{runSimulation}}.}

\item{states_step_ratio}{Discretization ratio to generate steps levels
between the reservoir capacity and zero}

\item{cvar_value}{from 0 to 1. the probability used in quantile method
to determine a bellman value which cvar_value all bellman values are equal or
less to it. (quantile(cvar_value))}

\item{force_final_level}{Binary. Whether final level should be constrained}

\item{penalty_final_level}{Penalties (for both bottom and top rule curves) to constrain final level}

\item{initial_traj}{Initial trajectory (used for other storages)}

\item{df_previous_cut}{Data frame containing previous estimations of cuts}

\item{list_areas_to_compute}{Vector of character. Areas for which to compute Bellman values. If \code{NULL}, all areas in \code{list_areas} are used.}
}
\value{
List containing aggregated water values and the data table with all years for the last iteration
}
\description{
Calculate Bellman values throughout iterations of Antares simulation and DP
Each simulation leads to a new reward estimation, which leads to new water values,
which leads to the off-line calculation in R of an optimal trajectory, which leads to
new controls to be evaluated which leads to a new simulation
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multistock_sequential_plaia.R
\name{getBellmanValuesSequentialMultiStockWithPlaia}
\alias{getBellmanValuesSequentialMultiStockWithPlaia}
\title{Calculate Bellman values sequentially, one area at a time.}
\usage{
getBellmanValuesSequentialMultiStockWithPlaia(
  list_areas,
  opts,
  nb_simulations,
  mcyears,
  penalty_low,
  penalty_high,
  states_step_ratio = 1/50,
  cvar_value = 1,
  penalty_final_level = NULL,
  initial_traj = NULL,
  list_areas_to_compute = NULL
)
}
\arguments{
\item{list_areas}{Vector of areas concerned by simulations.}

\item{opts}{List of study parameters returned by the function \code{antaresRead::setSimulationPath(simulation="input")} in input mode.}

\item{nb_simulations}{Number of controls to simulate}

\item{mcyears}{Vector of integer. Monte Carlo years used to compute water values.}

\item{penalty_low}{Double. Penalty for violating the bottom rule curve, comparable to the unsupplied energy cost.}

\item{penalty_high}{Double. Penalty for violating the top rule curve, comparable to the spilled energy cost.}

\item{states_step_ratio}{Double. Discretization ratio to generate steps levels
between the reservoir capacity and zero for which Bellman values are computed.}

\item{cvar_value}{Double from 0 to 1. The probability used in cvar method.}

\item{penalty_final_level}{Penalties (for both bottom and top rule curves) to force final level}

\item{initial_traj}{Initial trajectory (used for other storages)}

\item{list_areas_to_compute}{Vector of character. Areas for which to compute Bellman values. If \code{NULL}, all areas in \code{list_areas} are used.}
}
\value{
List containing aggregated water values, reward functions and optimal trajectories.
}
\description{
For each area, reward functions are first computed using
\code{calculateRewardsSimulationsWithPlaia()}. Bellman values are then
computed with \code{Grid_Matrix()}, and an optimal trajectory for the area
is derived using \code{getOptimalTrend()}.
The resulting trajectory is subsequently used to compute reward functions
for the next area in the sequence.
For areas where Bellman values have not yet been computed, either
\code{initial_traj} or a short-term trajectory is used as a fallback.
}

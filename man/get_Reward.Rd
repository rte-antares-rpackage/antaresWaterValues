% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_Reward.R
\name{get_Reward}
\alias{get_Reward}
\title{Compute reward functions}
\usage{
get_Reward(
  simulation_values = NULL,
  simulation_names = NULL,
  pattern = NULL,
  opts,
  correct_monotony = FALSE,
  method_old = TRUE,
  possible_controls = NULL,
  max_hydro_hourly = NULL,
  mcyears = "all",
  area = NULL,
  efficiency = NULL,
  expansion = F
)
}
\arguments{
\item{simulation_values}{A \code{dplyr::tibble()} with columns \code{"week"}, \code{"sim"}, \code{"u"} and \code{"mcYear"} (optional) that gives constraint values per week (and per scenario) used in each simulation.
Correspond to \code{simulation_values} output of \code{runWaterValuesSimulation()}.}

\item{simulation_names}{Vector of character. List of simulations names to use to compute reward.
Correspond to \code{simulation_names} output of \code{runWaterValuesSimulation()}.}

\item{pattern}{Character. A pattern to identify simulations.}

\item{opts}{List of study parameters returned by the function \code{antaresRead::setSimulationPath(simulation="input")} in input mode.}

\item{correct_monotony}{Binary. True to correct monotony of rewards if \code{method_old = TRUE}.}

\item{method_old}{Binary. Method to build reward function.}

\item{possible_controls}{If \code{method_old=FALSE}, controls for which to compute reward, generated by \code{constraint_generator()}.}

\item{max_hydro_hourly}{Maximum hourly pumping and generating power generated by the function \code{get_max_hydro()} with \code{timeStep="hourly"}.}

\item{mcyears}{Vector of integer. Monte Carlo years used to compute water values.}

\item{area}{Character. The Antares area concerned by water values computation.}

\item{efficiency}{Double between 0 and 1. Pumping efficiency ratio. Get it with \code{getPumpEfficiency()}.}

\item{expansion}{Binary. True if mode expansion (ie linear relaxation) of Antares is used to run simulations, argument passed to \code{\link[antaresEditObject]{runSimulation}}.
It is recommended to use mode expansion, it will be faster (only one iteration is done) and results will be smoother as the cost result will correspond to the linear relaxation of the problem.}
}
\value{
\item{reward}{A \code{dplyr::tibble()} with columns \code{"timeId"}, \code{"mcYear"}, \code{"control"} and \code{"reward"}. Reward functions for all weeks (\code{timeId}) and scenarios (\code{mcYear}).}
\item{local_reward}{Only if \code{method_old=FALSE}.
A \code{dplyr::tibble()} with columns \code{"week"}, \code{"mcYear"}, \code{"u"}, \code{"reward"} and \code{"simulation"}. All reward functions for all different simulations computed with \code{get_local_reward()} and \code{reward_offset()}.}
\item{simulation_names}{See arguments.}
\item{simulation_values}{See arguments.}
}
\description{
Compute reward functions for all weeks from 1 to 52 and for all scenarios in \code{mcyears} for the given area from simulations listed in \code{simulation_names}.
For a specific week and a specific scenario, the reward function is evaluated based on results of all simulations depending on the \code{method_old} chosen.
Mainly used in \code{Grid_Matrix()}.
}

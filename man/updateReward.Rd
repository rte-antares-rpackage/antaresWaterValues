% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterations_simulation_DP.R
\name{updateReward}
\alias{updateReward}
\title{Update df_rewards with latest simulation run,
used in \code{calculateBellmanWithIterativeSimulations}}
\usage{
updateReward(
  study_path,
  pumping,
  hours,
  controls,
  max_hydro,
  mcyears,
  area,
  pump_eff,
  u0,
  df_rewards,
  i
)
}
\arguments{
\item{study_path}{Path of Antares study, passed to \code{setSimulationPath}}

\item{pumping}{Binary, T if pumping possible}

\item{hours}{Vector of hours used to evaluate costs/rewards of pumping/generating,
passed to \code{get_local_reward}}

\item{controls}{Data frame containing possible transition for each week,
generated by the function \code{constraint_generator}}

\item{max_hydro}{data.frame {timeId,pump,turb} with maximum pumping and storing
powers for each hour,returned by the function  \code{get_max_hydro}}

\item{mcyears}{Vector of monte carlo years used to evaluate rewards}

\item{area}{Area with the reservoir}

\item{pump_eff}{Efficient ratio of pumping between 0 and 1}

\item{u0}{Constraint values per week used in the simulation,
generated by the function \code{constraint_generator}}

\item{df_rewards}{Data frame containing previous estimations of the reward function,
same format as the output of \code{reward_offset} with a column (n) containing the
iteration number}

\item{i}{Iteration number}
}
\value{
Updated data frame df_rewards
}
\description{
Update df_rewards with latest simulation run,
used in \code{calculateBellmanWithIterativeSimulations}
}

# Update df_rewards with latest simulation run, used in `calculateBellmanWithIterativeSimulations`

Update df_rewards with latest simulation run, used in
`calculateBellmanWithIterativeSimulations`

## Usage

``` r
updateReward(
  opts,
  pumping,
  controls,
  max_hydro_hourly,
  mcyears,
  area,
  pump_eff,
  u0,
  df_rewards,
  i,
  df_current_cuts,
  df_previous_cut
)
```

## Arguments

- opts:

  Path of Antares study, passed to `setSimulationPath`

- pumping:

  Binary, TRUE if pumping possible

- controls:

  Data frame containing possible transition for each week, generated by
  the function `constraint_generator`

- max_hydro_hourly:

  data.frame `timeId,pump,turb` with maximum pumping and storing powers
  for each hour,returned by the function `get_max_hydro`

- mcyears:

  Vector of monte carlo years used to evaluate rewards

- area:

  Area with the reservoir

- pump_eff:

  Efficient ratio of pumping between 0 and 1

- u0:

  Constraint values per week used in the simulation, generated by the
  function `constraint_generator`

- df_rewards:

  Data frame containing previous estimations of the reward function,
  same format as the output of `reward_offset` with a column (n)
  containing the iteration number

- i:

  Iteration number

- df_current_cuts:

  Data frame containing current estimations of cuts

- df_previous_cut:

  Data frame containing previous estimations of cuts

## Value

Updated data frame df_rewards
